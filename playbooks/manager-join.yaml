---
# =============================================================================
# Join control-plane (manager) node to the cluster
# =============================================================================
# Usage:
#   ansible-playbook -i hosts.yaml playbooks/manager-join.yaml --limit k8s-manager-2
# =============================================================================
# NOTE: First manager must be initialized with init-cluster.yaml
# =============================================================================

- name: Join manager to cluster
  hosts: managers
  become: true
  gather_facts: false

  tasks:
    - name: "[pre-check] Validate target"
      include_tasks: tasks/tasks-pre-check-secondary-manager.yaml

    # =========================================================================
    # GET CLUSTER INFO
    # =========================================================================
    - name: Get cluster nodes
      command: kubectl get nodes -o jsonpath='{.items[*].metadata.name}'
      delegate_to: "{{ master_manager_fact }}"
      register: cluster_nodes_result
      changed_when: false
      run_once: true

    - name: Set existing nodes as fact
      set_fact:
        existing_nodes_fact: "{{ cluster_nodes_result.stdout.split() }}"

    - name: Print existing nodes
      debug:
        msg: "Nodes in cluster: {{ existing_nodes_fact }}"
      run_once: true

    # =========================================================================
    # GENERATE CERTIFICATE KEY AND JOIN COMMAND
    # =========================================================================
    - name: Generate certificate-key
      command: kubeadm certs certificate-key
      delegate_to: "{{ master_manager_fact }}"
      register: cert_key_result
      changed_when: false
      run_once: true

    - name: Set certificate-key as fact
      set_fact:
        certificate_key_fact: "{{ cert_key_result.stdout }}"

    - name: Upload certs with certificate-key
      command: >-
        kubeadm init phase upload-certs --upload-certs
        --certificate-key {{ certificate_key_fact }}
      delegate_to: "{{ master_manager_fact }}"
      changed_when: false
      run_once: true

    - name: Print certificate-key
      debug:
        msg: "Certificate key: {{ certificate_key_fact }}"
      run_once: true

    - name: Generate join command
      command: >-
        kubeadm token create --print-join-command
        --certificate-key {{ certificate_key_fact }}
      delegate_to: "{{ master_manager_fact }}"
      register: join_command_result
      changed_when: false
      run_once: true

    - name: Set join command as fact
      set_fact:
        kubeadm_join_command_fact: "{{ join_command_result.stdout }}"

    - name: Print join command
      debug:
        msg: "{{ kubeadm_join_command_fact }}"
      run_once: true

    # =========================================================================
    # COPY ENCRYPTION CONFIG
    # =========================================================================
    - name: Fetch encryption config from master manager
      slurp:
        src: "{{ etcd_encryption_config_path }}"
      delegate_to: "{{ master_manager_fact }}"
      register: encryption_config_content
      run_once: true

    - name: Ensure PKI directory exists
      file:
        path: /etc/kubernetes/pki
        state: directory
        mode: '0755'

    - name: Copy encryption config to joining manager
      copy:
        dest: "{{ etcd_encryption_config_path }}"
        content: "{{ encryption_config_content.content | b64decode }}"
        mode: '0600'

    - name: Print encryption config status
      debug:
        msg: "Encryption config copied to {{ etcd_encryption_config_path }}"

    # =========================================================================
    # COPY VAULT CREDENTIALS (if exists)
    # =========================================================================
    - name: Check if Vault credentials exist on master manager
      stat:
        path: "{{ vault_creds_host_path }}"
      delegate_to: "{{ master_manager_fact }}"
      register: vault_creds_check
      run_once: true

    - name: Fetch Vault credentials from master manager
      slurp:
        src: "{{ vault_creds_host_path }}"
      delegate_to: "{{ master_manager_fact }}"
      register: vault_creds_content
      run_once: true
      when: vault_creds_check.stat.exists

    - name: Copy Vault credentials to joining manager
      copy:
        dest: "{{ vault_creds_host_path }}"
        content: "{{ vault_creds_content.content | b64decode }}"
        mode: '0600'
        owner: root
        group: root
      when: vault_creds_check.stat.exists

    - name: Print Vault credentials status
      debug:
        msg: "{{ 'Vault credentials copied to ' + vault_creds_host_path if vault_creds_check.stat.exists else 'Vault credentials not found on master - skipping' }}"

    # =========================================================================
    # JOIN MANAGER
    # =========================================================================
    - name: Check if already joined
      set_fact:
        is_already_joined_fact: "{{ new_hostname in existing_nodes_fact }}"

    - name: Skip if already joined
      debug:
        msg: "{{ new_hostname }} is already in cluster - skipping"
      when: is_already_joined_fact

    - name: Join cluster as control-plane
      command: >-
        {{ kubeadm_join_command_fact }}
        --apiserver-advertise-address {{ api_server_advertise_address }}
        --apiserver-bind-port {{ api_server_bind_port }}
      register: join_result
      when: not is_already_joined_fact

    - name: Print join result
      debug:
        msg: "{{ join_result.stdout_lines }}"
      when: not is_already_joined_fact and join_result.stdout_lines is defined

    # =========================================================================
    # VERIFY KUBELET
    # =========================================================================
    - name: Wait for kubelet
      pause:
        seconds: 15
      when: not is_already_joined_fact

    - name: Check kubelet status
      command: systemctl is-active kubelet
      register: kubelet_status
      changed_when: false
      failed_when: false

    - name: Assert kubelet is running
      assert:
        that:
          - kubelet_status.stdout == 'active'
        fail_msg: "kubelet is NOT running!"
        success_msg: "{{ new_hostname }}: kubelet running"

    # =========================================================================
    # CONFIGURE KUBECTL (for ansible_user)
    # =========================================================================
    - name: Create .kube directory for ansible_user
      file:
        path: "~{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'

    - name: Copy admin.conf to ansible_user .kube/config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "~{{ ansible_user }}/.kube/config"
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0600'

    # =========================================================================
    # REMOVE CONTROL-PLANE TAINT
    # =========================================================================
    - name: Remove NoSchedule taint from control-plane
      command: >-
        kubectl taint nodes {{ new_hostname }}
        node-role.kubernetes.io/control-plane:NoSchedule-
      register: untaint_result
      failed_when: false

    - name: Assert taint removal result
      assert:
        that:
          - untaint_result.rc == 0 or 'not found' in untaint_result.stderr
        fail_msg: "Failed to remove taint: {{ untaint_result.stderr }}"
        success_msg: >-
          {{ new_hostname }}: {{ 'Taint removed' if untaint_result.rc == 0 else 'Taint not exist' }}

    # =========================================================================
    # APPLY NODE LABELS
    # =========================================================================
    - name: Apply node labels
      command: "kubectl label nodes {{ new_hostname }} {{ item }} --overwrite"
      loop: "{{ node_labels | default([]) }}"
      when: node_labels is defined and node_labels | length > 0
      register: label_result
      failed_when: false

    - name: Print applied labels
      debug:
        msg: "Applied labels to {{ new_hostname }}: {{ node_labels }}"
      when: node_labels is defined and node_labels | length > 0

    # =========================================================================
    # APPLY LONGHORN DEFAULT NODE TAGS
    # =========================================================================
    - name: Apply Longhorn default node tags annotation
      command: >-
        kubectl annotate nodes {{ new_hostname }}
        node.longhorn.io/default-node-tags='{{ longhorn_default_tags | to_json }}'
        --overwrite
      when: longhorn_default_tags is defined and longhorn_default_tags | length > 0
      register: longhorn_tags_result
      failed_when: false

    - name: Print applied Longhorn tags
      debug:
        msg: "Applied Longhorn tags to {{ new_hostname }}: {{ longhorn_default_tags }}"
      when: longhorn_default_tags is defined and longhorn_default_tags | length > 0

    # =========================================================================
    # VERIFICATION
    # =========================================================================
    - name: Wait for node registration
      pause:
        seconds: 5
      run_once: true

    - name: Get cluster nodes
      command: kubectl get nodes -o wide
      delegate_to: "{{ master_manager_fact }}"
      register: final_nodes_result
      changed_when: false
      run_once: true

    - name: Print cluster status
      debug:
        msg:
          - "=== Cluster Nodes ==="
          - "{{ final_nodes_result.stdout_lines }}"
      run_once: true
