# =============================================================================
# Initialize Kubernetes cluster on master manager node
# =============================================================================
# Usage:
#   ansible-playbook -i hosts.yaml playbooks/init-cluster.yaml --limit k8s-manager-1
# =============================================================================
# NOTE: Run ONLY on the master manager (is_master: true)
# =============================================================================

---
# =============================================================================
# Play 0: Pre-check - validate target is the master manager
# =============================================================================
- name: Pre-check - validate target
  hosts: all
  gather_facts: false

  tasks:
    - name: Fail if no limit specified
      fail:
        msg: "Usage: ansible-playbook -i hosts.yaml playbooks/init-cluster.yaml --limit <master_manager>"
      when: ansible_limit is not defined
      run_once: true

    - name: Fail if not a manager node
      fail:
        msg: "ERROR: {{ inventory_hostname }} is not in 'managers' group"
      when: inventory_hostname not in groups['managers']

    - name: Fail if not the master manager
      fail:
        msg: "ERROR: {{ inventory_hostname }} does not have is_master: true"
      when: not (is_master | default(false))

    - name: Confirm target
      debug:
        msg: "Initializing cluster on master manager: {{ inventory_hostname }}"

# =============================================================================
# Play 1: Initialize Kubernetes cluster
# =============================================================================
- name: Initialize Kubernetes cluster
  hosts: managers
  become: true
  gather_facts: true

  tasks:
    # =========================================================================
    # PRE-CHECK
    # =========================================================================
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_already_init_pre_check

    - name: Print cluster status
      debug:
        msg: "{{ 'Cluster already init, skip' if kubeadm_already_init_pre_check.stat.exists else 'Cluster not init, proceeding...' }}"

    # =========================================================================
    # ETCD ENCRYPTION
    # =========================================================================
    - name: Ensure PKI directory exists
      file:
        path: /etc/kubernetes/pki
        state: directory
        mode: '0755'
      when: not kubeadm_already_init_pre_check.stat.exists

    - name: Generate ETCD encryption key
      shell: head -c 32 /dev/urandom | base64
      register: etcd_encryption_key_result
      when: not kubeadm_already_init_pre_check.stat.exists

    - name: Create ETCD encryption config
      copy:
        dest: "{{ etcd_encryption_config_path }}"
        mode: '0600'
        content: |
          apiVersion: apiserver.config.k8s.io/v1
          kind: EncryptionConfiguration
          resources:
            - resources: {{ etcd_encryption_resources | to_json }}
              providers:
                - aescbc:
                    keys:
                      - name: key{{ ansible_facts.date_time.epoch }}
                        secret: {{ etcd_encryption_key_result.stdout }}
                - identity: {}
      when: not kubeadm_already_init_pre_check.stat.exists

    - name: Print encryption key (SAVE THIS!)
      debug:
        msg:
          - "=== ETCD ENCRYPTION KEY ==="
          - "CRITICAL: Save this key securely! Without it, etcd backup is useless."
          - "Key: {{ etcd_encryption_key_result.stdout }}"
          - "Config path: {{ etcd_encryption_config_path }}"
      when: not kubeadm_already_init_pre_check.stat.exists

    # =========================================================================
    # KUBEADM CONFIG
    # =========================================================================
    - name: Build certSANs list
      set_fact:
        cert_sans: >-
          {{
            [haproxy_apiserver_lb_host, 'localhost', api_server_advertise_address] +
            groups['managers'] | map('extract', hostvars, 'api_server_advertise_address') | list +
            groups['managers'] | map('extract', hostvars, 'ansible_host') | list
          }}
      when: not kubeadm_already_init_pre_check.stat.exists

    - name: Create kubeadm config
      copy:
        dest: /root/kubeadm-config.yaml
        content: |
          apiVersion: kubeadm.k8s.io/v1beta4
          kind: InitConfiguration
          localAPIEndpoint:
            advertiseAddress: "{{ api_server_advertise_address }}"
            bindPort: {{ api_server_bind_port }}
          nodeRegistration:
            criSocket: "unix:///var/run/containerd/containerd.sock"
          ---
          apiVersion: kubeadm.k8s.io/v1beta4
          kind: ClusterConfiguration
          kubernetesVersion: "{{ k8s_full_version }}"
          controlPlaneEndpoint: "{{ haproxy_apiserver_lb_host }}:{{ haproxy_apiserver_lb_port }}"
          apiServer:
            extraArgs:
              - name: service-node-port-range
                value: "{{ node_port_start }}-{{ node_port_end }}"
              - name: encryption-provider-config
                value: "{{ etcd_encryption_config_path }}"
            extraVolumes:
              - name: encryption-config
                hostPath: "{{ etcd_encryption_config_path }}"
                mountPath: "{{ etcd_encryption_config_path }}"
                readOnly: true
                pathType: File
            certSANs: {{ cert_sans | to_json }}
          networking:
            serviceSubnet: "{{ service_subnet }}"
            podSubnet: "{{ pod_subnet }}"
            dnsDomain: "{{ dns_domain }}"
          controllerManager:
            extraArgs:
              - name: allocate-node-cidrs
                value: "false"
              - name: node-monitor-grace-period
                value: "{{ node_monitor_grace_period }}"
          ---
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          cgroupDriver: systemd
          containerLogMaxSize: {{ kubelet_container_log_max_size }}
          containerLogMaxFiles: {{ kubelet_container_log_max_files }}
          crashLoopBackOff:
            maxContainerRestartPeriod: {{ kubelet_crashloop_max_backoff }}
          ---
          apiVersion: kubeproxy.config.k8s.io/v1alpha1
          kind: KubeProxyConfiguration
          mode: ipvs
        mode: '0644'
      when: not kubeadm_already_init_pre_check.stat.exists

    # =========================================================================
    # KUBEADM INIT
    # =========================================================================
    - name: Init cluster with kubeadm
      command: >-
        kubeadm init
        --skip-phases=addon/kube-proxy
        --config /root/kubeadm-config.yaml
      register: kubeadm_init_result
      when: not kubeadm_already_init_pre_check.stat.exists

    - name: Print kubeadm init output
      debug:
        msg: "{{ kubeadm_init_result.stdout_lines }}"
      when: not kubeadm_already_init_pre_check.stat.exists

    # =========================================================================
    # CONFIGURE KUBECTL (for ansible_user)
    # =========================================================================
    - name: Create .kube directory for ansible_user
      file:
        path: "~{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'

    - name: Copy admin.conf to ansible_user .kube/config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "~{{ ansible_user }}/.kube/config"
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0600'

    # =========================================================================
    # CONFIGURE KUBECTL (for root)
    # =========================================================================
    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

    - name: Copy admin.conf to root .kube/config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: yes
        mode: '0600'

    # =========================================================================
    # REMOVE CONTROL-PLANE TAINT (optional)
    # =========================================================================
    - name: Remove NoSchedule taint from control-plane
      command: >-
        kubectl taint nodes {{ new_hostname }}
        node-role.kubernetes.io/control-plane:NoSchedule-
      register: untaint_result
      failed_when: false

    - name: Assert taint removal result
      assert:
        that:
          - untaint_result.rc == 0 or 'not found' in untaint_result.stderr
        fail_msg: "Failed to remove taint: {{ untaint_result.stderr }}"
        success_msg: >-
          {{ new_hostname }}: {{ 'Taint removed' if untaint_result.rc == 0 else 'Taint not exist' }}

    # =========================================================================
    # APPLY NODE LABELS
    # =========================================================================
    - name: Apply node labels
      command: "kubectl label nodes {{ new_hostname }} {{ item }} --overwrite"
      loop: "{{ node_labels | default([]) }}"
      when: node_labels is defined and node_labels | length > 0
      register: label_result
      failed_when: false

    - name: Print applied labels
      debug:
        msg: "Applied labels to {{ new_hostname }}: {{ node_labels }}"
      when: node_labels is defined and node_labels | length > 0

    # =========================================================================
    # APPLY LONGHORN DEFAULT NODE TAGS
    # =========================================================================
    - name: Apply Longhorn default node tags annotation
      command: >-
        kubectl annotate nodes {{ new_hostname }}
        node.longhorn.io/default-node-tags='{{ longhorn_default_tags | to_json }}'
        --overwrite
      when: longhorn_default_tags is defined and longhorn_default_tags | length > 0
      register: longhorn_tags_result
      failed_when: false

    - name: Print applied Longhorn tags
      debug:
        msg: "Applied Longhorn tags to {{ new_hostname }}: {{ longhorn_default_tags }}"
      when: longhorn_default_tags is defined and longhorn_default_tags | length > 0

    # =========================================================================
    # VERIFICATION
    # =========================================================================
    - name: Check cluster nodes
      command: kubectl get nodes -o wide
      register: nodes_check
      changed_when: false

    - name: Check cluster pods
      command: kubectl get pods -A
      register: pods_check
      changed_when: false

    - name: Print final status
      debug:
        msg:
          - "=== Cluster Initialized ==="
          - "Nodes:"
          - "{{ nodes_check.stdout_lines }}"
          - "Pods:"
          - "{{ pods_check.stdout_lines }}"
