---
# =============================================================================
# Remove worker node from Kubernetes cluster
# =============================================================================
# Usage:
#   ansible-playbook -i hosts.yaml playbooks/node-remove.yaml --limit k8s-worker-2
# =============================================================================
# Only works with nodes from 'workers' group
# =============================================================================

# =============================================================================
# Play 0: Pre-check - validate target is a worker node
# =============================================================================
- name: Pre-check - validate target
  hosts: all
  gather_facts: false

  tasks:
    - name: Fail if no limit specified
      fail:
        msg: "Usage: ansible-playbook -i hosts.yaml playbooks/node-remove.yaml --limit <worker_node>"
      when: ansible_limit is not defined
      run_once: true

    - name: Fail if not a worker node
      fail:
        msg: "ERROR: {{ inventory_hostname }} is not in 'workers' group"
      when: inventory_hostname not in groups['workers']

    - name: Confirm target
      debug:
        msg: "Removing worker node: {{ inventory_hostname }} ({{ new_hostname }})"

# =============================================================================
# Play 1: Delete node from cluster
# =============================================================================
- name: Delete node from cluster
  hosts: all
  become: true
  gather_facts: false

  tasks:
    - name: Find master manager
      set_fact:
        master_manager_fact: "{{ item }}"
      loop: "{{ groups['managers'] }}"
      when: hostvars[item].is_master | default(false)
      run_once: true

    - name: Fail if no master manager found
      fail:
        msg: "ERROR: No manager with is_master: true found in hosts.yaml"
      when: master_manager_fact is not defined
      run_once: true

    # Check if node exists in cluster
    - name: Get cluster nodes
      command: kubectl get nodes -o jsonpath='{.items[*].metadata.name}'
      delegate_to: "{{ master_manager_fact }}"
      register: cluster_nodes_result
      changed_when: false
      run_once: true

    - name: Set is_node_exists fact
      set_fact:
        is_node_exists: "{{ new_hostname in cluster_nodes_result.stdout.split() }}"

    - name: Skip if node not in cluster
      debug:
        msg: "{{ new_hostname }} is NOT in cluster - skipping delete"
      when: not is_node_exists

    # Delete node
    - name: Delete node from cluster
      command: kubectl delete node {{ new_hostname }}
      delegate_to: "{{ master_manager_fact }}"
      register: delete_result
      when: is_node_exists
      run_once: true

    - name: Print delete result
      debug:
        msg: "{{ delete_result.stdout }}"
      when: is_node_exists and delete_result.stdout is defined
      run_once: true

# =============================================================================
# Play 2: Verify node removed
# =============================================================================
- name: Verify node removed
  hosts: all
  become: true
  gather_facts: false

  tasks:
    - name: Find master manager
      set_fact:
        master_manager_fact: "{{ item }}"
      loop: "{{ groups['managers'] }}"
      when: hostvars[item].is_master | default(false)
      run_once: true

    - name: Get final cluster nodes
      command: kubectl get nodes -o wide
      delegate_to: "{{ master_manager_fact }}"
      register: final_nodes
      changed_when: false
      run_once: true

    - name: Print cluster status
      debug:
        msg:
          - "=== Node {{ new_hostname }} Removed ==="
          - "{{ final_nodes.stdout_lines }}"
      run_once: true

