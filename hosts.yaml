# =============================================================================
# Список серверов
# Заполни реальными данными
# =============================================================================

all:
  vars:
    # Explicitly set Python interpreter to avoid discovery warnings
    ansible_python_interpreter: /usr/bin/python3
    ansible_ssh_private_key_file: "~/.ssh/id_rsa" # for all playbooks
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no" # skip host key verification

    # SSH settings
    ssh_public_key_path: "~/.ssh/id_rsa.pub" # for setup-ssh-keys.yaml

    # Path for local helm charts on remote server
    remote_charts_dir: "/opt/helm-charts"

    # LLVM/Clang version for Cilium eBPF
    llvm_version: 20

    # Kubernetes version (MAJOR.MINOR only, without patch)
    k8s_version: "1.34"

    # Kubernetes full version for kubeadm (with patch)
    k8s_full_version: "v1.34.0"

    # Kubeadm config path (used by init-cluster and update-apiserver-sans)
    kubeadm_config_path: "/etc/kubernetes/kubeadm-config.yaml"

    # Containerd version
    containerd_version: "2.2.1"

    # Force regenerate containerd config (set to true to update config after containerd upgrade)
    force_containerd_config_regenerate: false

    # runc version
    runc_version: "1.4.0"

    # CNI plugins version
    cni_plugins_version: "1.9.0"

    # Cluster networking
    service_subnet: "10.128.0.0/12"
    pod_subnet: "10.64.0.0/10"
    dns_domain: "cluster.local"
    node_port_start: 1
    node_port_end: 50000

    # kube-controller-manager settings
    node_monitor_grace_period: "30s" # default: 40s (for faster node failure detection)

    # Node drain settings
    node_drain_timeout: "10m"

    # Watchdog (softdog) for medik8s
    softdog_timeout: 30 # seconds before reboot if not pinged

    # Helm version
    helm_version: "3.19.2"

    # k9s version
    k9s_version: "0.50.18"

    # APT repository packages (required for adding k8s repo)
    apt_repo_package_list:
      - apt-transport-https
      - ca-certificates
      - curl
      - gpg

    # Kubernetes packages
    k8s_package_list:
      - kubelet
      - kubeadm
      - kubectl

    # Kubernetes base kernel modules
    k8s_module_list:
      - overlay
      - br_netfilter

    # Kubernetes sysctl parameters (all must be = 1)
    k8s_sysctl_list:
      - net.bridge.bridge-nf-call-iptables
      - net.bridge.bridge-nf-call-ip6tables
      - net.ipv4.ip_forward

    # Longhorn kernel modules
    longhorn_modules:
      - iscsi_tcp
      - dm_crypt

    # Longhorn required packages
    longhorn_packages:
      - open-iscsi
      - nfs-common
      - cryptsetup
      - dmsetup

    # Cilium kernel modules (ALL are critical)
    cilium_modules:
      - cls_bpf
      - sch_ingress
      - udp_tunnel
      - ip6_udp_tunnel
      - geneve
      - vxlan
      - algif_hash
      - af_alg
      - sch_fq
      - ip_set
      - ip_set_hash_ip
      - xt_set
      - xt_comment
      - xt_TPROXY
      - xt_CT
      - xt_mark
      - xt_socket
      - xfrm_algo
      - xfrm_user
      - esp4
      - esp6
      - ipcomp
      - ipcomp6
      - xfrm4_tunnel
      - xfrm6_tunnel
      - tunnel4
      - tunnel6

    # ------
    # HAProxy API Server LB (static pod)
    # ------
    haproxy_apiserver_lb_image: "haproxy:3.3.1-alpine3.23"
    haproxy_apiserver_lb_host: "127.0.0.1"
    haproxy_apiserver_lb_port: 16443
    haproxy_apiserver_lb_healthz_port: 16444

    # ETCD encryption at rest
    etcd_encryption_resources:
      - secrets
      - configmaps
    etcd_encryption_config_path: "/etc/kubernetes/pki/encryption-config.yaml"
    # ETCD key rotation state files
    etcd_rotation_state_dir: "/etc/kubernetes/pki"
    etcd_rotation_state_step1: "{{ etcd_rotation_state_dir }}/rotation-state-step1.yaml"
    etcd_rotation_state_step2: "{{ etcd_rotation_state_dir }}/rotation-state-step2.yaml"
    etcd_rotation_state_step4: "{{ etcd_rotation_state_dir }}/rotation-state-step4.yaml"

    # Kubelet logging
    kubelet_container_log_max_size: "100Mi"
    kubelet_container_log_max_files: 5

    # CrashLoopBackOff max backoff (1s - 300s) (default 300s = 5min)
    kubelet_crashloop_max_backoff: "2m"

    # ------
    # Kubelet Image Garbage Collection
    # Очистка неиспользуемых Docker/containerd images
    # Docs: https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/
    # ------
    # Процент использования диска, при котором НАЧИНАЕТСЯ очистка images (default: 85)
    kubelet_image_gc_high_threshold: 85
    # Процент использования диска, до которого очищать images (default: 80)
    kubelet_image_gc_low_threshold: 75
    # Минимальное время с последнего использования image перед удалением (default: 2m)
    kubelet_image_minimum_gc_age: "5m"

    # ------
    # Kubelet Eviction — Soft Thresholds
    # Отложенное удаление pods с grace period (SIGTERM)
    # Срабатывает раньше hard, даёт время на graceful shutdown
    # ------
    # Включить soft eviction (default: false в kubernetes)
    kubelet_eviction_soft_enabled: true
    # Минимум свободной RAM для soft eviction (default: не установлен)
    kubelet_eviction_soft_memory_available: "1Gi"
    # Минимум свободного места на root filesystem (default: не установлен)
    kubelet_eviction_soft_nodefs_available: "15%"
    # Минимум свободного места для images (default: не установлен)
    kubelet_eviction_soft_imagefs_available: "15%"

    # ------
    # Kubelet Eviction — Soft Grace Periods
    # Время ожидания перед началом eviction (если ресурс не восстановился)
    # 11:00:00 — memory.available = 900Mi (< 1Gi soft threshold)
    #  ├─ Kubelet обнаруживает нарушение soft threshold
    #  ├─ Нода получает condition: MemoryPressure=True
    #  ├─ Scheduler ПЕРЕСТАЁТ планировать новые pods на эту ноду
    #  └─ Запускается ТАЙМЕР на 2 минуты (grace period)
    # 
    # 11:00:00 - 11:02:00 — ПЕРИОД ОЖИДАНИЯ
    #  ├─ Kubelet продолжает мониторить memory каждые 10 сек
    #  ├─ Pods продолжают работать
    #  └─ Приложения имеют шанс освободить память
    #
    # 11:02:00 — Таймер истёк, проверяем memory
    #  └─ ВАРИАНТ A: memory.available >= 1Gi
    #     ├─ Soft eviction ОТМЕНЯЕТСЯ
    #     ├─ MemoryPressure=False
    #     └─ Scheduler снова планирует pods на эту ноду
    #
    #  └─ ВАРИАНТ B: memory.available всё ещё < 1Gi
    #     ├─ Начинается EVICTION
    #     ├─ Kubelet выбирает pods для удаления (по QoS + usage)
    #     ├─ Pods получают SIGTERM (graceful shutdown)
    #     ├─ Ждёт terminationGracePeriodSeconds (из pod spec)
    #     └─ После graceful shutdown — pod удалён
    # ------
    # Grace period для memory.available
    kubelet_eviction_soft_grace_period_memory: "2m"
    # Grace period для nodefs.available
    kubelet_eviction_soft_grace_period_nodefs: "2m"
    # Grace period для imagefs.available
    kubelet_eviction_soft_grace_period_imagefs: "2m"

    # ------
    # Kubelet Eviction — Hard Thresholds
    # Немедленное удаление pods при критической нехватке ресурсов (SIGKILL)
    # Docs: https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/
    # ------
    # 11:00:00 — memory = 900Mi → Soft eviction таймер запущен
    # 11:00:30 — memory = 400Mi (< 500Mi HARD threshold!)
    #  ├─ HARD eviction срабатывает НЕМЕДЛЕННО
    #  ├─ Soft eviction таймер игнорируется
    #  ├─ Pods получают SIGKILL (без grace period)
    #  └─ Принудительное удаление
    # ------
    # Включить hard eviction (default: true в kubernetes)
    kubelet_eviction_hard_enabled: true
    # Минимум свободной RAM, иначе eviction pods (default: 100Mi)
    kubelet_eviction_hard_memory_available: "500Mi"
    # Минимум свободного места на root filesystem (default: 10%)
    kubelet_eviction_hard_nodefs_available: "10%"
    # Минимум свободных inodes на root filesystem (default: 5%)
    kubelet_eviction_hard_nodefs_inodes_free: "5%"
    # Минимум свободного места для images (default: 15%)
    kubelet_eviction_hard_imagefs_available: "10%"
    # Минимум свободных PID, защита от fork bomb (default: не установлен)
    kubelet_eviction_hard_pid_available: "1000"

    # VPN IPs for vpn-only middleware
    vpn_ips:
      - "1.2.3.4/32"

    # ------
    # CRDs wait configuration (shared for all apps)
    # ------
    crds_wait:
      timeout: "60s"
      retries: 15
      delay: 5

    # ------
    # Cilium
    # ------
    cilium_version: "1.18.4"
    cilium_namespace: "cilium"
    cilium_mask_size: 21 # Cilium IPAM settings (pods per node)
    cilium_operator_replicas: 1 # number of replicas for cilium-operator (should be 1 for single-node cluster)
    cilium_hubble_ui_domain: "cilium-hubble-ui-k8s-v2.drawapp.ru" # Hubble UI domain
    cilium_hubble_ui_vpn_only_enabled: false
    cilium_hubble_ui_https_secret_name: "cert-cilium-hubble-ui-domain-com"
    cilium_helm_timeout: "5m"
    cilium_config_helm_timeout: "2m"
    cilium_post_config_helm_timeout: "2m"
    cilium_daemonset_rollout_timeout: "300s"
    cilium_deployment_rollout_timeout: "120s"

    # ------
    # Cert-manager
    # ------
    cert_manager_version: "1.19.2"
    cert_manager_namespace: "cert-manager"
    cert_manager_acme_email: "some-email-123@gmail.com"
    cert_manager_cluster_issuer: "cluster-issuer-acme-prod"
    cert_manager_helm_timeout: "5m"
    cert_manager_config_helm_timeout: "2m"
    cert_manager_rollout_timeout: "180s"

    # ------
    # External Secrets Operator
    # ------
    external_secrets_namespace: "external-secrets"
    external_secrets_version: "v1.2.1"
    external_secrets_helm_timeout: "5m"
    external_secrets_rollout_timeout: "300s"

    # ------
    # Traefik
    # ------
    traefik_chart_version: "38.0.2"
    traefik_version: "v3.6.2"
    traefik_namespace: "traefik-master"
    traefik_daemonset_name: "traefik-master"
    traefik_ingress_class: "traefik-master-lb"
    traefik_web_entrypoint: "web" # WARNING: DO NOT CHANGE - hardcoded in helm chart
    traefik_websecure_entrypoint: "websecure" # WARNING: DO NOT CHANGE - hardcoded in helm chart
    traefik_dashboard_domain: "traefik-dashboard-k8s-v2.drawapp.ru"
    traefik_dashboard_vpn_only_enabled: false
    traefik_dashboard_https_secret_name: "cert-traefik-dashboard-domain-com"
    traefik_dashboard_service_name: "traefik-dashboard"
    traefik_helm_timeout: "5m"
    traefik_rollout_timeout: "300s"

    # ------
    # HAProxy (Ingress Controller)
    # ------
    haproxy_chart_version: "1.44.0"
    haproxy_namespace: "haproxy-master"
    haproxy_daemonset_name: "haproxy-master"
    haproxy_ingress_class: "haproxy-master-lb"
    haproxy_helm_timeout: "5m"
    haproxy_config_helm_timeout: "2m"
    haproxy_rollout_timeout: "300s"

    # ------
    # OLM v0 (Operator Lifecycle Manager)
    # ------
    olm_namespace: "olm" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml
    olm_operators_namespace: "operators" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml
    olm_helm_timeout: "5m"
    olm_rollout_timeout: "300s"

    # ------
    # medik8s (Node Health Check + Self Node Remediation)
    # ------
    medik8s_watchdog_file_path: "/dev/watchdog"
    medik8s_api_check_interval: "6s"
    medik8s_api_server_timeout: "3s"
    medik8s_max_api_error_threshold: 2
    medik8s_peer_dial_timeout: "3s"
    medik8s_peer_request_timeout: "5s"
    medik8s_peer_update_interval: "15m"
    medik8s_safe_time_to_assume_rebooted: 120
    medik8s_remediation_strategy: "OutOfServiceTaint"
    medik8s_nhc_unhealthy_duration: "15s"
    medik8s_nhc_workers_min_healthy: "51%"
    medik8s_nhc_control_plane_min_healthy: "51%"
    medik8s_config_helm_timeout: "2m"

    # ------
    # Longhorn
    # ------
    longhorn_chart_version: "1.10.1"
    longhorn_namespace: "longhorn-system" # DO NOT CHANGE
    longhorn_ui_domain: "longhorn-ui-k8s-v2.drawapp.ru"
    longhorn_ui_vpn_only_enabled: false
    longhorn_ui_https_secret_name: "cert-longhorn-ui-domain-com"
    # Longhorn AZ list (Для каждой AZ создается свой набор StorageClass)
    # Эти az должны совпадать с az в разделе managers и workers (в нижней части файла hosts.yaml)
    # цель: все реплики одного volume должны быть в одной зоне (в рамках одного StorageClass)
    # Название StorageClass = "{{ $az }}-major-multi-best-effort"
    # NodeSelector = "lh-major-volume,{{ $az }}"
    longhorn_az_list:
      - "lh-az-fra-1"
      - "lh-az-waw-1"
      - "lh-az-swe-1"
    # Longhorn ESO integration
    longhorn_eso:
      sa_name: "eso-longhorn" # Из списка ниже (vault_roles)
      role_name: "eso-longhorn" # Из списка ниже (vault_roles)
      secret_store_name: "eso-vault-longhorn"
      kv_engine_path: "eso-secret"
      secrets:
        - external_secret_name: "s3-backup" # Название ExternalSecret (который будет создан в этом namespace)
          target_secret_name: "longhorn-s3-backup-creds" # Название секрета в Kubernetes (который будет создан в этом namespace)
          vault_path: "/longhorn-system/s3-backup" # Full path in Vault
          refresh_interval: "1m"
    longhorn_helm_timeout: "5m"
    longhorn_config_helm_timeout: "2m"
    longhorn_manager_rollout_timeout: "300s"
    longhorn_ui_rollout_timeout: "120s"

    # ------
    # Vault (HashiCorp)
    # 1 namespace = 1 SA + 1 SecretStore (для ESO)
    # sa_name - это название ServiceAccount (который будет создан в этом namespace)
    # role_name - это название Role (Внутри Vault - auth/kubernetes/role/${role_name})
    # secret_store_name - это название SecretStore (который будет создан в этом namespace)
    # ---
    # secrets - это массив ExternalSecret (который будет создан в этом namespace)
    # name - это название ExternalSecret
    # target_secret - это название секрета в Kubernetes (который будет создан в этом namespace)
    # vault_path - это full path в Vault
    # refresh_interval - это интервал обновления секрета
    # ------
    vault_namespace: "vault"
    vault_image_tag: "1.21.2"
    vault_domain: "vault-k8s-v2.drawapp.ru"
    vault_https_secret_name: "cert-vault-domain-com"
    vault_vpn_only_enabled: false
    vault_storage_class: "lh-major-single-best-effort"
    vault_storage_size: "1Gi"
    # Vault unseal
    vault_key_shares: 3
    vault_key_threshold: 2
    vault_auto_unseal_schedule: "*/5 * * * *"  # CronJob schedule for auto-unseal
    vault_creds_host_path: "/etc/kubernetes/vault-unseal.json"  # Path to credentials file on control-plane nodes
    vault_creds_init_tmp_path: "/etc/kubernetes/vault-creds-init-tmp.json"  # Temp file for interrupted init
    vault_creds_rotate_tmp_path: "/etc/kubernetes/vault-creds-rotate-tmp.json"  # Temp file for interrupted rotate
    # Vault Kubernetes Auth
    vault_kubernetes_host: "https://kubernetes.default.svc:443"
    # Vault KV engines to create (array of mount paths)
    vault_kv_engines:
      - "secret"
      - "eso-secret"
    # Vault ESO integration (for storing vault credentials)
    vault_eso:
      sa_name: "eso-vault-self"
      role_name: "eso-vault-self"
      secret_store_name: "eso-vault-self"
      kv_engine_path: "eso-secret"
      secrets:
        - external_secret_name: "eso-vault-self-creds"
          target_secret_name: "eso-vault-self-creds"
          vault_path: "/vault/vault-self/creds"
          refresh_interval: "1m"
          is_master: true

    # ---------
    # VAULT POLICIES (Single Source of Truth)
    # secret | eso-secret = это secret_engine (из массива vault_kv_engines)
    # data = это данные по секретам (path: secret/data/vault/*)
    # metadata = это metadata по секретам (path: secret/metadata/vault/*)
    # ---------
    vault_policies:
      # ---------
      # Vault Admin (full access for vault-admin SA)
      # ---------
      - name: eso-vault-admin
        paths:
          - path: secret/data/*
            capabilities: [create, read, update, delete, list]
          - path: secret/metadata/*
            capabilities: [create, read, update, delete, list]
          - path: eso-secret/data/*
            capabilities: [create, read, update, delete, list]
          - path: eso-secret/metadata/*
            capabilities: [create, read, update, delete, list]
          - path: sys/policies/acl/*
            capabilities: [create, read, update, delete, list]
          - path: auth/*
            capabilities: [create, read, update, delete, list]

      # ---------
      # Vault self credentials (for ESO to read vault root credentials)
      # ---------
      - name: eso-vault-self
        paths:
          - path: eso-secret/data/vault/*
            capabilities: [read, list]
          - path: eso-secret/metadata/vault/*
            capabilities: [read, list]

      # ---------
      # Longhorn (S3 backup credentials)
      # ---------
      - name: eso-longhorn-read
        paths:
          - path: eso-secret/data/longhorn-system/*
            capabilities: [read, list]
          - path: eso-secret/metadata/longhorn-system/*
            capabilities: [read, list]

      # ---------
      # GitLab (all secrets: postgresql, redis, minio, gitlab root)
      # ---------
      - name: eso-gitlab-read
        paths:
          - path: eso-secret/data/gitlab/*
            capabilities: [read, list]
          - path: eso-secret/metadata/gitlab/*
            capabilities: [read, list]

      # ---------
      # ArgoCD (SSH repo-creds)
      # ---------
      - name: eso-argocd-read
        paths:
          - path: eso-secret/data/argocd/*
            capabilities: [read, list]
          - path: eso-secret/metadata/argocd/*
            capabilities: [read, list]

    # ---------
    # VAULT ROLES (Single Source of Truth)
    # ---------
    vault_roles:
      # ---------
      # Vault Admin (for writing secrets from Ansible)
      # ---------
      - name: eso-vault-admin
        namespace: vault
        sa_name: eso-vault-admin
        policies:
          - eso-vault-admin

      # ---------
      # Vault ESO root (for reading vault credentials)
      # ---------
      - name: eso-vault-self
        namespace: vault
        sa_name: eso-vault-self
        policies:
          - eso-vault-self

      # ---------
      # Longhorn
      # ---------
      - name: eso-longhorn
        namespace: longhorn-system
        sa_name: eso-longhorn
        policies:
          - eso-longhorn-read

      # ---------
      # GitLab + Gitlab-minio + gitlab-runner
      # ---------
      - name: eso-gitlab
        namespace: gitlab
        sa_name: eso-gitlab
        policies:
          - eso-gitlab-read

      # ---------
      # ArgoCD
      # ---------
      - name: eso-argocd
        namespace: argocd
        sa_name: eso-argocd
        policies:
          - eso-argocd-read

    # ------
    vault_helm_timeout: "5m"
    vault_config_helm_timeout: "2m"
    vault_rollout_timeout: "300s"

    # ------
    # GitLab (Production Helm)
    # ------
    gitlab_namespace: "gitlab"
    gitlab_chart_version: "8.11.8"  # GitLab 17.11
    # Base domain - используется для генерации внутренних URL (CI/CD переменные, clone URL, webhooks)
    # https: true - потому что внешний трафик приходит по HTTPS (TLS терминируется на Traefik + cert-manager)
    gitlab_base_domain: "gitlab-k8s-v2.drawapp.ru"
    gitlab_domain: "gitlab-k8s-v2.drawapp.ru"
    gitlab_registry_domain: "gitlab-registry-k8s-v2.drawapp.ru"
    gitlab_pages_domain: "gitlab-pages-k8s-v2.drawapp.ru"
    gitlab_minio_console_domain: "gitlab-minio-k8s-v2.drawapp.ru"
    gitlab_minio_api_domain: "gitlab-minio-api-k8s-v2.drawapp.ru"
    gitlab_minio_secret_name: "cert-gitlab-minio-domain-com"
    gitlab_ssh_port_external: 3714

    # GitLab Ingress settings (custom ingress via post chart)
    gitlab_https_secret_name: "cert-gitlab-v2-domain-com"
    gitlab_vpn_only_enabled: false

    # GitLab PostgreSQL
    gitlab_postgresql_image: "postgres:16-alpine"
    gitlab_postgresql_storage_class: "lh-major-multi-best-effort"
    gitlab_postgresql_storage_size: "4Gi"

    # GitLab Redis
    gitlab_redis_image: "redis/redis-stack-server:latest"
    gitlab_redis_storage_class: "lh-major-single-best-effort"
    gitlab_redis_storage_size: "1Gi"

    # GitLab MinIO
    gitlab_minio_image: "minio/minio:latest"
    gitlab_minio_root_user: "admin"
    gitlab_minio_storage_class: "lh-major-single-best-effort"
    gitlab_minio_storage_size: "2Gi"

    # GitLab MinIO S3 connection settings (used by ESO template for gitlab_minio_connection secret)
    gitlab_minio_s3_endpoint: "http://svc-gitlab-minio:9000"
    gitlab_minio_s3_region: "us-east-1"
    gitlab_minio_s3_path_style: true

    # GitLab Webservice
    gitlab_webservice_memory_request: "2Gi"
    gitlab_webservice_cpu_request: "250m"
    gitlab_webservice_memory_limit: "4Gi"

    # GitLab Sidekiq
    gitlab_sidekiq_memory_request: "1Gi"
    gitlab_sidekiq_cpu_request: "250m"
    gitlab_sidekiq_memory_limit: "3Gi"

    # GitLab Gitaly
    gitlab_gitaly_storage_size: "4Gi"
    gitlab_gitaly_storage_class: "lh-major-single-best-effort"

    # GitLab ESO integration
    gitlab_eso:
      sa_name: "eso-gitlab"
      role_name: "eso-gitlab"
      secret_store_name: "eso-vault-gitlab"
      kv_engine_path: "eso-secret"
      secrets:
        - external_secret_name: "eso-gitlab-postgresql"
          target_secret_name: "eso-gitlab-postgresql"
          vault_path: "/gitlab/postgresql/creds"
          refresh_interval: "1m"
          type: "postgresql"
        - external_secret_name: "eso-gitlab-redis"
          target_secret_name: "eso-gitlab-redis"
          vault_path: "/gitlab/redis/creds"
          refresh_interval: "1m"
          type: "redis"
        - external_secret_name: "eso-gitlab-minio-root"
          target_secret_name: "eso-gitlab-minio-root"
          vault_path: "/gitlab/minio/root"
          refresh_interval: "1m"
          type: "minio_root"
        - external_secret_name: "eso-gitlab-minio-registry"
          target_secret_name: "eso-gitlab-minio-registry"
          vault_path: "/gitlab/minio/registry"
          refresh_interval: "1m"
          type: "minio_registry"
        # SPECIAL: gitlab_registry_minio_connection uses ESO template to generate registry storage config
        - external_secret_name: "eso-gitlab-registry-minio-connection"
          target_secret_name: "eso-gitlab-registry-minio-connection"
          vault_path: "/gitlab/minio/registry"
          refresh_interval: "1m"
          type: "gitlab_registry_minio_connection"
        # SPECIAL: gitlab_minio_connection uses ESO template to generate s3_connection YAML
        - external_secret_name: "eso-gitlab-minio-connection"
          target_secret_name: "eso-gitlab-minio-connection"
          vault_path: "/gitlab/minio/registry"
          refresh_interval: "1m"
          type: "gitlab_minio_connection"
        - external_secret_name: "eso-gitlab-minio-runner-cache"
          target_secret_name: "eso-gitlab-minio-runner-cache"
          vault_path: "/gitlab/minio/runner-cache"
          refresh_interval: "1m"
          type: "minio_runner_cache"
        - external_secret_name: "eso-gitlab-root"
          target_secret_name: "eso-gitlab-root"
          vault_path: "/gitlab/gitlab/root"
          refresh_interval: "1m"
          type: "gitlab_root"
    # ------
    gitlab_helm_timeout: "15m"
    gitlab_pre_helm_timeout: "5m"
    gitlab_storage_helm_timeout: "5m"
    gitlab_minio_rollout_timeout: "300s"
    gitlab_rollout_timeout: "600s"
    gitlab_storage_rollout_timeout: "300s"

    # ------
    # ArgoCD
    # ------
    argocd_namespace: "argocd" # DO NOT CHANGE - hardcoded in ClusterRoleBindings
    argocd_ui_domain: "argocd-ui-k8s.domain.com"
    argocd_rpc_domain: "argocd-rpc-k8s.domain.com"
    argocd_https_secret_name: "cert-argocd-domain-com"
    argocd_vpn_only_enabled: false
    argocd_reconciliation_timeout: "30s"
    argocd_reconciliation_jitter: "10s"
    argocd_session_duration: "120h"
    # ArgoCD ESO integration (SSH repo-creds secret from Vault)
    argocd_eso:
      sa_name: "eso-argocd"
      role_name: "eso-argocd"
      secret_store_name: "eso-vault-argocd"
      kv_engine_path: "eso-secret"
      secrets:
        - external_secret_name: "eso-argocd-repo-creds"
          target_secret_name: "eso-git-ops-root-key-internal"
          vault_path: "/argocd/repo-creds"
          refresh_interval: "1m"
    # ------
    argocd_helm_timeout: "5m"
    argocd_components_rollout_timeout: "120s"

    # ------
    # ArgoCD git-ops GitLab setup
    # ------
    argocd_git_ops_gitlab_group: "server"
    argocd_git_ops_gitlab_repo: "git-ops"
    argocd_git_ops_path: "argocd-system"
    argocd_git_ops_target_revision: "master"
    argocd_git_ops_gitlab_deploy_key_name: "argocd-read-only"
    argocd_git_ops_retry_backoff_max_duration: "30s"
    argocd_git_ops_destination_server: "https://kubernetes.default.svc"
    argocd_git_ops_repo_url: "ssh://git@svc-gitlab.gitlab.svc.cluster.local:22/server/git-ops.git"
    argocd_git_ops_source_repos: ["*"]
    argocd_git_ops_destinations:
      - name: "*"
        namespace: "*"
        server: "*"
    argocd_git_ops_retry_backoff_factor: 1
    argocd_git_ops_retry_limit: 5
    argocd_git_ops_retry_backoff_duration: "20s"
    argocd_git_ops_pre_helm_timeout: "3m"
    argocd_git_ops_install_helm_timeout: "3m"
    argocd_git_ops_secret_wait_retries: 24
    argocd_git_ops_secret_wait_delay: 5

  children:
    managers:
      hosts:
        k8s-manager-1:
          ansible_host: 158.160.8.137
          ansible_user: ubuntu
          ansible_password: "some-password-123"
          new_hostname: k8s-manager-1
          is_master: true
          # internal_ip - internal network interface IP (same as api_server_advertise_address for managers)
          internal_ip: "10.129.0.10"
          # api_server_advertise_address - IP for kube-apiserver to listen on and advertise to cluster
          # BareMetal: usually same as ansible_host (public IP)
          # Cloud (AWS/Yandex): usually private VPC IP (check with: ip route get 1)
          api_server_advertise_address: "10.129.0.10"
          api_server_bind_port: 6443
          longhorn_default_tags:
            - "lh-az-fra-1"
            - "lh-manager"
            - "lh-major-volume"

        # k8s-manager-2:
        #   ansible_host: 84.201.165.81
        #   ansible_user: ubuntu
        #   ansible_password: "some-password-456"
        #   new_hostname: k8s-manager-2
        #   internal_ip: "10.129.0.4"
        #   api_server_advertise_address: "10.129.0.4"
        #   api_server_bind_port: 6443
        #   longhorn_default_tags:
        #     - "lh-az-fra-1"
        #     - "lh-manager"
        #     - "lh-major-volume"

    workers:
      hosts:
        k8s-worker-1:
          ansible_host: 51.250.17.248
          ansible_user: ubuntu
          ansible_password: "some-password-789"
          new_hostname: k8s-worker-1
          internal_ip: "10.129.0.24"
          node_labels:
            - "node-role.kubernetes.io/worker="
          longhorn_default_tags:
            - "lh-az-fra-1"
            - "lh-worker"
            - "lh-major-volume"
            - "lh-minor-volume"

        k8s-worker-2:
          ansible_host: 178.154.198.58
          ansible_user: ubuntu
          ansible_password: "some-password-101"
          new_hostname: k8s-worker-2
          internal_ip: "10.129.0.25"
          node_labels:
            - "node-role.kubernetes.io/worker="
          longhorn_default_tags:
            - "lh-az-fra-1"
            - "lh-worker"
            - "lh-major-volume"
            - "lh-minor-volume"
