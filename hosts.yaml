# =============================================================================
# Список серверов
# Заполни реальными данными
# =============================================================================

all:
  vars:
    # Explicitly set Python interpreter to avoid discovery warnings
    ansible_python_interpreter: /usr/bin/python3
    ansible_ssh_private_key_file: "~/.ssh/id_rsa" # for all playbooks
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no" # skip host key verification

    # SSH settings
    ssh_public_key_path: "~/.ssh/id_rsa.pub" # for setup-ssh-keys.yaml

    # Path for local helm charts on remote server
    remote_charts_dir: "/opt/helm-charts"

    # LLVM/Clang version for Cilium eBPF
    llvm_version: 20

    # Kubernetes version (MAJOR.MINOR only, without patch)
    k8s_version: "1.34"

    # Kubernetes full version for kubeadm (with patch)
    k8s_full_version: "v1.34.0"

    # Containerd version
    containerd_version: "2.2.1"

    # Force regenerate containerd config (set to true to update config after containerd upgrade)
    force_containerd_config_regenerate: false

    # runc version
    runc_version: "1.4.0"

    # CNI plugins version
    cni_plugins_version: "1.9.0"

    # Cluster networking
    service_subnet: "10.128.0.0/12"
    pod_subnet: "10.64.0.0/10"
    dns_domain: "cluster.local"
    node_port_start: 1
    node_port_end: 50000

    # kube-controller-manager settings
    node_monitor_grace_period: "30s" # default: 40s (for faster node failure detection)

    # Watchdog (softdog) for medik8s
    softdog_timeout: 30 # seconds before reboot if not pinged

    # Helm version
    helm_version: "3.19.2"

    # k9s version
    k9s_version: "0.50.18"

    # APT repository packages (required for adding k8s repo)
    apt_repo_package_list:
      - apt-transport-https
      - ca-certificates
      - curl
      - gpg

    # Kubernetes packages
    k8s_package_list:
      - kubelet
      - kubeadm
      - kubectl

    # Kubernetes base kernel modules
    k8s_module_list:
      - overlay
      - br_netfilter

    # Kubernetes sysctl parameters (all must be = 1)
    k8s_sysctl_list:
      - net.bridge.bridge-nf-call-iptables
      - net.bridge.bridge-nf-call-ip6tables
      - net.ipv4.ip_forward

    # Longhorn kernel modules
    longhorn_modules:
      - iscsi_tcp
      - dm_crypt

    # Longhorn required packages
    longhorn_packages:
      - open-iscsi
      - nfs-common
      - cryptsetup
      - dmsetup

    # Cilium kernel modules (ALL are critical)
    cilium_modules:
      - cls_bpf
      - sch_ingress
      - udp_tunnel
      - ip6_udp_tunnel
      - geneve
      - vxlan
      - algif_hash
      - af_alg
      - sch_fq
      - ip_set
      - ip_set_hash_ip
      - xt_set
      - xt_comment
      - xt_TPROXY
      - xt_CT
      - xt_mark
      - xt_socket
      - xfrm_algo
      - xfrm_user
      - esp4
      - esp6
      - ipcomp
      - ipcomp6
      - xfrm4_tunnel
      - xfrm6_tunnel
      - tunnel4
      - tunnel6

    # ------
    # HAProxy API Server LB (static pod)
    # ------
    haproxy_apiserver_lb_image: "haproxy:3.3.1-alpine3.23"
    haproxy_apiserver_lb_host: "127.0.0.1"
    haproxy_apiserver_lb_port: 16443
    haproxy_apiserver_lb_healthz_port: 16444

    # ETCD encryption at rest
    etcd_encryption_resources:
      - secrets
      - configmaps
    etcd_encryption_config_path: "/etc/kubernetes/pki/encryption-config.yaml"

    # Kubelet logging
    kubelet_container_log_max_size: "100Mi"
    kubelet_container_log_max_files: 5

    # CrashLoopBackOff max backoff (1s - 300s) (default 300s = 5min)
    kubelet_crashloop_max_backoff: "2m"

    # ------
    # Kubelet Image Garbage Collection
    # Очистка неиспользуемых Docker/containerd images
    # Docs: https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/
    # ------
    # Процент использования диска, при котором НАЧИНАЕТСЯ очистка images (default: 85)
    kubelet_image_gc_high_threshold: 85
    # Процент использования диска, до которого очищать images (default: 80)
    kubelet_image_gc_low_threshold: 75
    # Минимальное время с последнего использования image перед удалением (default: 2m)
    kubelet_image_minimum_gc_age: "5m"

    # ------
    # Kubelet Eviction — Soft Thresholds
    # Отложенное удаление pods с grace period (SIGTERM)
    # Срабатывает раньше hard, даёт время на graceful shutdown
    # ------
    # Включить soft eviction (default: false в kubernetes)
    kubelet_eviction_soft_enabled: true
    # Минимум свободной RAM для soft eviction (default: не установлен)
    kubelet_eviction_soft_memory_available: "1Gi"
    # Минимум свободного места на root filesystem (default: не установлен)
    kubelet_eviction_soft_nodefs_available: "15%"
    # Минимум свободного места для images (default: не установлен)
    kubelet_eviction_soft_imagefs_available: "15%"

    # ------
    # Kubelet Eviction — Soft Grace Periods
    # Время ожидания перед началом eviction (если ресурс не восстановился)
    # 11:00:00 — memory.available = 900Mi (< 1Gi soft threshold)
    #  ├─ Kubelet обнаруживает нарушение soft threshold
    #  ├─ Нода получает condition: MemoryPressure=True
    #  ├─ Scheduler ПЕРЕСТАЁТ планировать новые pods на эту ноду
    #  └─ Запускается ТАЙМЕР на 2 минуты (grace period)
    # 
    # 11:00:00 - 11:02:00 — ПЕРИОД ОЖИДАНИЯ
    #  ├─ Kubelet продолжает мониторить memory каждые 10 сек
    #  ├─ Pods продолжают работать
    #  └─ Приложения имеют шанс освободить память
    #
    # 11:02:00 — Таймер истёк, проверяем memory
    #  └─ ВАРИАНТ A: memory.available >= 1Gi
    #     ├─ Soft eviction ОТМЕНЯЕТСЯ
    #     ├─ MemoryPressure=False
    #     └─ Scheduler снова планирует pods на эту ноду
    #
    #  └─ ВАРИАНТ B: memory.available всё ещё < 1Gi
    #     ├─ Начинается EVICTION
    #     ├─ Kubelet выбирает pods для удаления (по QoS + usage)
    #     ├─ Pods получают SIGTERM (graceful shutdown)
    #     ├─ Ждёт terminationGracePeriodSeconds (из pod spec)
    #     └─ После graceful shutdown — pod удалён
    # ------
    # Grace period для memory.available
    kubelet_eviction_soft_grace_period_memory: "2m"
    # Grace period для nodefs.available
    kubelet_eviction_soft_grace_period_nodefs: "2m"
    # Grace period для imagefs.available
    kubelet_eviction_soft_grace_period_imagefs: "2m"

    # ------
    # Kubelet Eviction — Hard Thresholds
    # Немедленное удаление pods при критической нехватке ресурсов (SIGKILL)
    # Docs: https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/
    # ------
    # 11:00:00 — memory = 900Mi → Soft eviction таймер запущен
    # 11:00:30 — memory = 400Mi (< 500Mi HARD threshold!)
    #  ├─ HARD eviction срабатывает НЕМЕДЛЕННО
    #  ├─ Soft eviction таймер игнорируется
    #  ├─ Pods получают SIGKILL (без grace period)
    #  └─ Принудительное удаление
    # ------
    # Включить hard eviction (default: true в kubernetes)
    kubelet_eviction_hard_enabled: true
    # Минимум свободной RAM, иначе eviction pods (default: 100Mi)
    kubelet_eviction_hard_memory_available: "500Mi"
    # Минимум свободного места на root filesystem (default: 10%)
    kubelet_eviction_hard_nodefs_available: "10%"
    # Минимум свободных inodes на root filesystem (default: 5%)
    kubelet_eviction_hard_nodefs_inodes_free: "5%"
    # Минимум свободного места для images (default: 15%)
    kubelet_eviction_hard_imagefs_available: "10%"
    # Минимум свободных PID, защита от fork bomb (default: не установлен)
    kubelet_eviction_hard_pid_available: "1000"

    # VPN IPs for vpn-only middleware
    vpn_ips:
      - "1.2.3.4/32"

    # ------
    # CRDs wait configuration (shared for all apps)
    # ------
    crds_wait:
      timeout: "60s"
      retries: 15
      delay: 5

    # ------
    # Cilium
    # ------
    cilium_version: "1.18.4"
    cilium_namespace: "cilium"
    cilium_mask_size: 21 # Cilium IPAM settings (pods per node)
    cilium_operator_replicas: 1 # number of replicas for cilium-operator (should be 1 for single-node cluster)
    cilium_hubble_ui_domain: "cilium-hubble-ui-k8s-v2.drawapp.ru" # Hubble UI domain
    cilium_hubble_ui_vpn_only_enabled: false
    cilium_hubble_ui_https_secret_name: "cert-cilium-hubble-ui-domain-com"

    # ------
    # Cert-manager
    # ------
    cert_manager_version: "1.19.2"
    cert_manager_namespace: "cert-manager"
    cert_manager_acme_email: "some-email-123@gmail.com"
    cert_manager_cluster_issuer: "cluster-issuer-acme-prod"

    # ------
    # External Secrets Operator
    # ------
    external_secrets_namespace: "ns-external-secrets"
    external_secrets_version: "1.2.1"

    # ------
    # Traefik
    # ------
    traefik_chart_version: "38.0.2"
    traefik_version: "3.6.2"
    traefik_namespace: "ns-traefik-master"
    traefik_daemonset_name: "traefik-master"
    traefik_ingress_class: "traefik-master-lb"
    traefik_web_entrypoint: "web" # WARNING: DO NOT CHANGE - hardcoded in helm chart
    traefik_websecure_entrypoint: "websecure" # WARNING: DO NOT CHANGE - hardcoded in helm chart
    traefik_dashboard_domain: "traefik-dashboard-k8s-v2.drawapp.ru"
    traefik_dashboard_vpn_only_enabled: false
    traefik_dashboard_https_secret_name: "cert-traefik-dashboard-domain-com"
    traefik_dashboard_service_name: "traefik-dashboard"

    # ------
    # HAProxy (Ingress Controller)
    # ------
    haproxy_chart_version: "1.44.0"
    haproxy_namespace: "ns-haproxy-master"
    haproxy_daemonset_name: "haproxy-master"
    haproxy_ingress_class: "haproxy-master-lb"

    # ------
    # OLM v0 (Operator Lifecycle Manager)
    # ------
    olm_namespace: "olm" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml
    olm_operators_namespace: "operators" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml

    # ------
    # medik8s (Node Health Check + Self Node Remediation)
    # ------
    medik8s_watchdog_file_path: "/dev/watchdog"
    medik8s_api_check_interval: "6s"
    medik8s_api_server_timeout: "3s"
    medik8s_max_api_error_threshold: 2
    medik8s_peer_dial_timeout: "3s"
    medik8s_peer_request_timeout: "5s"
    medik8s_peer_update_interval: "15m"
    medik8s_safe_time_to_assume_rebooted: 120
    medik8s_remediation_strategy: "OutOfServiceTaint"
    medik8s_nhc_unhealthy_duration: "15s"
    medik8s_nhc_workers_min_healthy: "51%"
    medik8s_nhc_control_plane_min_healthy: "51%"

    # ------
    # Longhorn
    # ------
    longhorn_chart_version: "1.10.1"
    longhorn_namespace: "longhorn-system" # DO NOT CHANGE
    longhorn_ui_domain: "longhorn-ui-k8s-v2.drawapp.ru"
    longhorn_ui_vpn_only_enabled: false
    longhorn_ui_https_secret_name: "cert-longhorn-ui-domain-com"
    # Longhorn AZ list (Для каждой AZ создается свой набор StorageClass)
    # Эти az должны совпадать с az в разделе managers и workers (в нижней части файла hosts.yaml)
    # цель: все реплики одного volume должны быть в одной зоне (в рамках одного StorageClass)
    longhorn_az_list:
      - "fr-1"
      - "de-1"
    # Longhorn ESO integration
    longhorn_eso:
      sa_name: "eso-longhorn" # Из списка ниже (vault_roles)
      role_name: "eso-longhorn" # Из списка ниже (vault_roles)
      secret_store_name: "eso-vault-longhorn"
      kv_engine_path: "secret"
      secrets:
        - external_secret_name: "s3-backup" # Название ExternalSecret (который будет создан в этом namespace)
          target_secret_name: "longhorn-s3-backup-creds" # Название секрета в Kubernetes (который будет создан в этом namespace)
          vault_path: "/longhorn-system/s3-backup" # Full path in Vault
          refresh_interval: "1m"

    # ------
    # Vault (HashiCorp)
    # 1 namespace = 1 SA + 1 SecretStore (для ESO)
    # sa_name - это название ServiceAccount (который будет создан в этом namespace)
    # role_name - это название Role (Внутри Vault - auth/kubernetes/role/role_name)
    # secret_store_name - это название SecretStore (который будет создан в этом namespace)
    # ---
    # secrets - это массив ExternalSecret (который будет создан в этом namespace)
    # name - это название ExternalSecret
    # target_secret - это название секрета в Kubernetes (который будет создан в этом namespace)
    # vault_path - это full path в Vault
    # refresh_interval - это интервал обновления секрета
    # ------
    vault_namespace: "ns-vault"
    vault_image_tag: "1.21.2"
    vault_domain: "vault-k8s-v2.drawapp.ru"
    vault_https_secret_name: "cert-vault-domain-com"
    vault_vpn_only_enabled: false
    vault_storage_class: "lh-major-single-best-effort"
    vault_storage_size: "1Gi"
    # Vault unseal
    vault_key_shares: 3
    vault_key_threshold: 2
    vault_auto_unseal_schedule: "*/5 * * * *"  # CronJob schedule for auto-unseal
    vault_creds_host_path: "/etc/kubernetes/vault-unseal.json"  # Path to credentials file on control-plane nodes
    vault_creds_init_tmp_path: "/etc/kubernetes/vault-creds-init-tmp.json"  # Temp file for interrupted init
    vault_creds_rotate_tmp_path: "/etc/kubernetes/vault-creds-rotate-tmp.json"  # Temp file for interrupted rotate
    # Vault Kubernetes Auth
    vault_kubernetes_host: "https://kubernetes.default.svc:443"
    # Vault KV engines to create (array of mount paths)
    vault_kv_engines:
      - "secret"
      - "eso-secret"
    # Vault ESO integration (for storing vault credentials)
    vault_eso:
      sa_name: "eso-vault-self"
      role_name: "eso-vault-self"
      secret_store_name: "eso-vault-self"
      kv_engine_path: "secret"
      secrets:
        - external_secret_name: "eso-vault-self-creds"
          target_secret_name: "eso-vault-self-creds"
          vault_path: "/ns-vault/vault-self/creds"
          refresh_interval: "1m"
          is_master: true

    # ---------
    # VAULT POLICIES (Single Source of Truth)
    # secret = это secret_engine (из массива vault_kv_engines)
    # data = это данные по секретам (path: secret/data/ns-vault/*)
    # metadata = это metadata по секретам (path: secret/metadata/ns-vault/*)
    # ---------
    vault_policies:
      # ---------
      # Vault self credentials (for ESO to read vault root credentials)
      # ---------
      - name: eso-vault-self
        paths:
          - path: secret/data/ns-vault/*
            capabilities: [read, list]
          - path: secret/metadata/ns-vault/*
            capabilities: [read, list]

      # ---------
      # Vault Admin (full access for vault-admin SA)
      # ---------
      - name: eso-vault-admin
        paths:
          - path: secret/data/*
            capabilities: [create, read, update, delete, list]
          - path: secret/metadata/*
            capabilities: [create, read, update, delete, list]
          - path: sys/policies/acl/*
            capabilities: [create, read, update, delete, list]
          - path: auth/*
            capabilities: [create, read, update, delete, list]

      # ---------
      # Longhorn (S3 backup credentials)
      # ---------
      - name: eso-longhorn-read
        paths:
          - path: secret/data/longhorn-system/*
            capabilities: [read, list]
          - path: secret/metadata/longhorn-system/*
            capabilities: [read, list]

      # ---------
      # GitLab (root password)
      # ---------
      - name: eso-gitlab-read
        paths:
          - path: secret/data/ns-gitlab/*
            capabilities: [read, list]
          - path: secret/metadata/ns-gitlab/*
            capabilities: [read, list]

      # ---------
      # GitLab MinIO (root credentials, registry keys, runner-cache keys)
      # ---------
      - name: eso-gitlab-minio-read
        paths:
          - path: secret/data/ns-gitlab/*
            capabilities: [read, list]
          - path: secret/metadata/ns-gitlab/*
            capabilities: [read, list]

      # ---------
      # ArgoCD (admin password)
      # ---------
      - name: eso-argocd-read
        paths:
          - path: secret/data/argocd/*
            capabilities: [read, list]
          - path: secret/metadata/argocd/*
            capabilities: [read, list]

    # ---------
    # VAULT ROLES (Single Source of Truth)
    # ---------
    vault_roles:
      # ---------
      # Vault ESO root (for reading vault credentials)
      # ---------
      - name: eso-vault-self
        namespace: ns-vault
        sa_name: eso-vault-self
        policies:
          - eso-vault-self

      # ---------
      # Vault Admin (for writing secrets from Ansible)
      # ---------
      - name: eso-vault-admin
        namespace: ns-vault
        sa_name: eso-vault-admin
        policies:
          - eso-vault-admin

      # ---------
      # Longhorn
      # ---------
      - name: eso-longhorn
        namespace: longhorn-system
        sa_name: eso-longhorn
        policies:
          - eso-longhorn-read

      # ---------
      # GitLab
      # ---------
      - name: eso-gitlab
        namespace: ns-gitlab
        sa_name: eso-gitlab
        policies:
          - eso-gitlab-read

      # ---------
      # GitLab MinIO
      # ---------
      - name: eso-gitlab-minio
        namespace: ns-gitlab
        sa_name: eso-gitlab-minio
        policies:
          - eso-gitlab-minio-read

    # ------
    # GitLab MinIO
    # ------
    gitlab_minio_image: "minio/minio:RELEASE.2025-04-22T22-12-26Z"
    gitlab_minio_storage_size: "2Gi"
    gitlab_minio_domain: "gitlab-minio-k8s-v2.drawapp.ru"
    gitlab_minio_api_domain: "gitlab-minio-api-k8s-v2.drawapp.ru"
    gitlab_minio_https_secret_name: "cert-gitlab-minio-domain-com"
    gitlab_minio_root_user: "admin"
    gitlab_minio_storage_class: "lh-major-single-best-effort"
    gitlab_minio_vpn_only_enabled: false
    gitlab_minio_service_url: "http://svc-gitlab-minio.ns-gitlab.svc.cluster.local:9000"
    # GitLab MinIO ESO integration
    # NOTE: Before running playbook, create secrets in Vault with bucket_name field:
    #   - ns-gitlab/gitlab-minio/registry: bucket_name=registry
    #   - ns-gitlab/gitlab-minio/runner-cache: bucket_name=runner-cache
    gitlab_minio_eso:
      sa_name: "eso-gitlab-minio" # From vault_roles list below
      role_name: "eso-gitlab-minio" # From vault_roles list below
      secret_store_name: "eso-vault-gitlab-minio"
      kv_engine_path: "secret"
      secrets:
        - external_secret_name: "eso-gitlab-minio-root-user"
          target_secret_name: "eso-gitlab-minio-root-user"
          vault_path: "/ns-gitlab/gitlab-minio/root-user"
          refresh_interval: "1m"
          type: "MASTER"
        - external_secret_name: "eso-gitlab-minio-registry"
          target_secret_name: "eso-gitlab-minio-registry"
          vault_path: "/ns-gitlab/gitlab-minio/registry"
          refresh_interval: "1m"
          type: "REGISTRY"
        - external_secret_name: "eso-gitlab-minio-runner-cache"
          target_secret_name: "eso-gitlab-minio-runner-cache"
          vault_path: "/ns-gitlab/gitlab-minio/runner-cache"
          refresh_interval: "1m"
          type: "RUNNER_CACHE"

    # ------
    # GitLab
    # ------
    gitlab_namespace: "ns-gitlab"
    gitlab_image: "gitlab/gitlab-ce:16.11.10-ce.0"
    gitlab_domain: "gitlab-k8s.domain.com"
    gitlab_registry_domain: "gitlab-registry-k8s.domain.com"
    gitlab_pages_domain: "gitlab-pages-k8s.domain.com"
    gitlab_https_secret_name: "cert-gitlab-domain-com"
    gitlab_vpn_only_enabled: false
    gitlab_ssh_port_external: 3714
    gitlab_storage_class: "lh-major-multi-best-effort"
    gitlab_storage_class_logs: "lh-major-worker-multi-best-effort"
    gitlab_storage_config_size: "1Gi"
    gitlab_storage_data_size: "20Gi"
    gitlab_storage_logs_size: "5Gi"
    gitlab_resources_requests_memory: "2G"
    gitlab_resources_limits_memory: "3G"
    # GitLab ESO integration
    gitlab_eso:
      sa_name: "eso-gitlab"
      role_name: "eso-gitlab"
      secret_store_name: "eso-vault-gitlab"
      kv_engine_path: "secret"
      secrets:
        - external_secret_name: "eso-gitlab-root-user"
          target_secret_name: "eso-gitlab-root-user"
          vault_path: "/ns-gitlab/gitlab/root-user"
          refresh_interval: "1m"
          type: "MASTER"

    # ------
    # ArgoCD
    # ------
    argocd_namespace: "argocd" # DO NOT CHANGE - hardcoded in ClusterRoleBindings
    argocd_ui_domain: "argocd-ui-k8s.domain.com"
    argocd_rpc_domain: "argocd-rpc-k8s.domain.com"
    argocd_https_secret_name: "cert-argocd-domain-com"
    argocd_vpn_only_enabled: false
    argocd_reconciliation_timeout: "30s"
    argocd_reconciliation_jitter: "10s"
    argocd_session_duration: "120h"
    argocd_gitlab_internal_url: "ssh://git@svc-gitlab.ns-gitlab.svc.cluster.local:22/server/git-ops.git"
    argocd_gitlab_ssh_key: |
      -----BEGIN OPENSSH PRIVATE KEY-----
      REPLACE_WITH_YOUR_KEY
      -----END OPENSSH PRIVATE KEY-----

    # ------
    # Helm timeouts
    # ------
    # Cilium
    cilium_helm_timeout: "5m"
    cilium_config_helm_timeout: "2m"
    cilium_post_config_helm_timeout: "2m"
    # Cert-manager
    cert_manager_helm_timeout: "5m"
    cert_manager_config_helm_timeout: "2m"
    # Traefik
    traefik_helm_timeout: "5m"
    # HAProxy
    haproxy_helm_timeout: "5m"
    haproxy_config_helm_timeout: "2m"
    # OLM
    olm_helm_timeout: "5m"
    # Medik8s
    medik8s_config_helm_timeout: "2m"
    # Longhorn
    longhorn_helm_timeout: "5m"
    longhorn_config_helm_timeout: "2m"
    # External Secrets
    external_secrets_helm_timeout: "5m"
    # Vault
    vault_helm_timeout: "5m"
    vault_config_helm_timeout: "2m"
    # GitLab MinIO
    gitlab_minio_helm_timeout: "5m"
    # GitLab
    gitlab_helm_timeout: "10m"
    gitlab_config_helm_timeout: "2m"
    # ArgoCD
    argocd_helm_timeout: "5m"

    # ------
    # kubectl rollout/wait timeouts
    # ------
    # Cilium
    cilium_daemonset_rollout_timeout: "300s"
    cilium_deployment_rollout_timeout: "120s"
    # Cert-manager
    cert_manager_rollout_timeout: "180s"
    # Traefik
    traefik_rollout_timeout: "300s"
    # HAProxy
    haproxy_rollout_timeout: "300s"
    # OLM
    olm_rollout_timeout: "300s"
    # Longhorn
    longhorn_manager_rollout_timeout: "300s"
    longhorn_ui_rollout_timeout: "120s"
    # External Secrets
    external_secrets_rollout_timeout: "300s"
    # Vault
    vault_rollout_timeout: "300s"
    # GitLab MinIO
    gitlab_minio_rollout_timeout: "300s"
    # GitLab
    gitlab_rollout_timeout: "600s"
    # ArgoCD
    argocd_components_rollout_timeout: "120s"

  children:
    managers:
      hosts:
        k8s-manager-1:
          ansible_host: 158.160.1.1
          ansible_user: ubuntu
          ansible_password: "some-password-123"
          new_hostname: k8s-manager-1
          is_master: true
          # internal_ip - internal network interface IP (same as api_server_advertise_address for managers)
          internal_ip: "10.129.0.27"
          # api_server_advertise_address - IP for kube-apiserver to listen on and advertise to cluster
          # BareMetal: usually same as ansible_host (public IP)
          # Cloud (AWS/Yandex): usually private VPC IP (check with: ip route get 1)
          api_server_advertise_address: "158.160.1.1"
          api_server_bind_port: 6443
          longhorn_default_tags:
            - "lh-az-fr-1"
            - "lh-manager"
            - "lh-major-volume"

        # k8s-manager-2:
        #   ansible_host: 5.6.7.8
        #   ansible_user: root
        #   ansible_password: "some-password-456"
        #   new_hostname: k8s-manager-2
        #   api_server_advertise_address: "5.6.7.8"
        #   api_server_bind_port: 6443
        #   internal_ip: "5.6.7.8"
        #   longhorn_default_tags:
        #     - "lh-az-fr-1"
        #     - "lh-manager"
        #     - "lh-major-volume"

    workers:
      hosts:
        k8s-worker-1:
          ansible_host: 158.160.95.133
          ansible_user: ubuntu
          ansible_password: "some-password-789"
          new_hostname: k8s-worker-1
          internal_ip: "10.129.0.16"
          node_labels:
            - "node-role.kubernetes.io/worker="
          longhorn_default_tags:
            - "lh-az-fr-1"
            - "lh-worker"
            - "lh-major-volume"
            - "lh-minor-volume"

        # k8s-worker-2:
        #   ansible_host: 13.14.15.16
        #   ansible_user: root
        #   ansible_password: "some-password-101"
        #   new_hostname: k8s-worker-2
        #   node_labels:
        #     - "node-role.kubernetes.io/worker="
        #   longhorn_default_tags:
        #     - "lh-az-fr-1"
        #     - "lh-worker"
        #     - "lh-major-volume"
        #     - "lh-minor-volume"
