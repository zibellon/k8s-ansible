# =============================================================================
# Список серверов
# Заполни реальными данными
# =============================================================================

all:
  vars:
    # Explicitly set Python interpreter to avoid discovery warnings
    ansible_python_interpreter: /usr/bin/python3
    ansible_ssh_private_key_file: "~/.ssh/id_rsa" # for all playbooks
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no" # skip host key verification

    # SSH settings
    ssh_public_key_path: "~/.ssh/id_rsa.pub" # for setup-ssh-keys.yaml

    # LLVM/Clang version for Cilium eBPF
    llvm_version: 20

    # Kubernetes version (MAJOR.MINOR only, without patch)
    k8s_version: "1.34"

    # Kubernetes full version for kubeadm (with patch)
    k8s_full_version: "v1.34.0"

    # Containerd version
    containerd_version: "2.2.1"

    # Force regenerate containerd config (set to true to update config after containerd upgrade)
    force_containerd_config_regenerate: false

    # runc version
    runc_version: "1.4.0"

    # CNI plugins version
    cni_plugins_version: "1.9.0"

    # Cluster networking
    service_subnet: "10.128.0.0/12"
    pod_subnet: "10.64.0.0/10"
    dns_domain: "cluster.local"
    node_port_start: 1
    node_port_end: 50000

    # kube-controller-manager settings
    node_monitor_grace_period: "30s"   # default: 40s (for faster node failure detection)

    # Watchdog (softdog) for medik8s
    softdog_timeout: 30  # seconds before reboot if not pinged

    # Helm version
    helm_version: "3.19.2"

    # Path for local helm charts on remote server
    remote_charts_dir: "/opt/helm-charts"

    # APT repository packages (required for adding k8s repo)
    apt_repo_package_list:
      - apt-transport-https
      - ca-certificates
      - curl
      - gpg

    # Kubernetes packages
    k8s_package_list:
      - kubelet
      - kubeadm
      - kubectl

    # Kubernetes base kernel modules
    k8s_module_list:
      - overlay
      - br_netfilter

    # Kubernetes sysctl parameters (all must be = 1)
    k8s_sysctl_list:
      - net.bridge.bridge-nf-call-iptables
      - net.bridge.bridge-nf-call-ip6tables
      - net.ipv4.ip_forward

    # Longhorn kernel modules
    longhorn_modules:
      - iscsi_tcp
      - dm_crypt

    # Longhorn required packages
    longhorn_packages:
      - open-iscsi
      - nfs-common
      - cryptsetup
      - dmsetup

    # Cilium kernel modules (ALL are critical)
    cilium_modules:
      - cls_bpf
      - sch_ingress
      - udp_tunnel
      - ip6_udp_tunnel
      - geneve
      - vxlan
      - algif_hash
      - af_alg
      - sch_fq
      - ip_set
      - ip_set_hash_ip
      - xt_set
      - xt_comment
      - xt_TPROXY
      - xt_CT
      - xt_mark
      - xt_socket
      - xfrm_algo
      - xfrm_user
      - esp4
      - esp6
      - ipcomp
      - ipcomp6
      - xfrm4_tunnel
      - xfrm6_tunnel
      - tunnel4
      - tunnel6

    # ------
    # HAProxy API Server LB (static pod)
    # ------
    haproxy_apiserver_lb_image: "haproxy:3.3.1-alpine3.23"
    haproxy_apiserver_lb_host: "127.0.0.1"
    haproxy_apiserver_lb_port: 16443
    haproxy_apiserver_lb_healthz_port: 16444

    # VPN IPs for vpn-only middleware
    vpn_ips:
      - "1.2.3.4/32"

    # ------
    # Cilium
    # ------
    cilium_version: "1.18.4"
    cilium_namespace: "cilium"
    cilium_mask_size: 21 # Cilium IPAM settings (pods per node)
    cilium_operator_replicas: 1 # number of replicas for cilium-operator (should be 1 for single-node cluster)
    cilium_hubble_ui_domain: "cilium-hubble-ui-k8s.domain.com" # Hubble UI domain
    cilium_hubble_ui_vpn_only_enabled: false
    cilium_hubble_ui_https_secret_name: "cert-cilium-hubble-ui-domain-com"

    # ------
    # Cert-manager
    # ------
    cert_manager_version: "1.19.2"
    cert_manager_namespace: "cert-manager"
    cert_manager_acme_email: "some-email-123@gmail.com"
    cert_manager_cluster_issuer: "cluster-issuer-acme-prod"

    # ------
    # Traefik
    # ------
    traefik_version: "3.6.2" # Used for CRD download
    traefik_namespace: "ns-traefik-master"
    traefik_daemonset_name: "traefik-master"
    traefik_ingress_class: "traefik-master-lb"
    traefik_web_entrypoint: "traefik-web"
    traefik_websecure_entrypoint: "traefik-websecure"
    traefik_dashboard_domain: "traefik-dashboard-k8s.domain.com"
    traefik_dashboard_vpn_only_enabled: false
    traefik_dashboard_https_secret_name: "cert-traefik-dashboard-domain-com"

    # ------
    # HAProxy (Ingress Controller)
    # ------
    haproxy_chart_version: "1.44.0"
    haproxy_namespace: "ns-haproxy-master"
    haproxy_daemonset_name: "haproxy-master"
    haproxy_ingress_class: "haproxy-master-lb"

    # ------
    # OLM v0 (Operator Lifecycle Manager)
    # ------
    olm_namespace: "olm" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml
    olm_operators_namespace: "operators" # DO NOT CHANGE - hardcoded in templates/olm-v0-install.yaml

    # ------
    # medik8s (Node Health Check + Self Node Remediation)
    # ------
    medik8s_watchdog_file_path: "/dev/watchdog"
    medik8s_api_check_interval: "6s"
    medik8s_api_server_timeout: "3s"
    medik8s_max_api_error_threshold: 2
    medik8s_peer_dial_timeout: "3s"
    medik8s_peer_request_timeout: "5s"
    medik8s_peer_update_interval: "15m"
    medik8s_safe_time_to_assume_rebooted: 120
    medik8s_remediation_strategy: "OutOfServiceTaint"
    medik8s_nhc_unhealthy_duration: "15s"
    medik8s_nhc_workers_min_healthy: "51%"
    medik8s_nhc_control_plane_min_healthy: "51%"

    # ------
    # Longhorn
    # ------
    longhorn_chart_version: "1.10.1"
    longhorn_namespace: "longhorn-system" # DO NOT CHANGE
    longhorn_ui_domain: "longhorn-ui-k8s-v2.drawapp.ru"
    longhorn_ui_vpn_only_enabled: false
    longhorn_ui_https_secret_name: "cert-longhorn-ui-domain-com"
    longhorn_s3_access_key_id: "secret_123"
    longhorn_s3_secret_access_key: "secret_456"
    longhorn_s3_endpoint: "https://s3.your-provider.com"

    # ------
    # Infisical
    # ------
    infisical_namespace: "ns-infisical"
    infisical_chart_version: "v0.151.0"
    infisical_domain: "infisical-k8s.domain.com"
    infisical_https_secret_name: "cert-infisical-domain-com"
    infisical_vpn_only_enabled: false
    infisical_replicas: 1
    infisical_storage_class: "lh-major-worker-multi-best-effort"
    infisical_postgres_image: "postgres:17.5-alpine3.22"
    infisical_postgres_storage_size: "10Gi"
    infisical_postgres_port: 5432
    infisical_postgres_database: "infisical"
    infisical_postgres_username: "postgres"
    infisical_redis_image: "redis/redis-stack:7.4.0-v8"
    infisical_redis_storage_size: "2Gi"
    infisical_redis_port: 6379
    infisical_bootstrap_organization: "MyOrg"
    infisical_bootstrap_email: "admin@example.com"

    # ------
    # External Secrets
    # ------
    external_secrets_namespace: "ns-external-secrets"
    external_secrets_version: "1.2.1"

    # ------
    # GitLab MinIO
    # ------
    gitlab_minio_image: "minio/minio:RELEASE.2025-04-22T22-12-26Z"
    gitlab_minio_storage_size: "5Gi"
    gitlab_minio_domain: "gitlab-minio-k8s.domain.com"
    gitlab_minio_api_domain: "gitlab-minio-api-k8s.domain.com"
    gitlab_minio_https_secret_name: "cert-gitlab-minio-domain-com"
    gitlab_minio_root_user: "admin"
    gitlab_minio_root_password: "pass_123"
    gitlab_minio_storage_class: "lh-major-worker-multi-best-effort"
    gitlab_minio_registry_bucket_name: "registry"
    gitlab_minio_registry_keys_secret_name: "secret-gitlab-minio-registry-keys"
    gitlab_minio_registry_key_name: "gitlab-registry-key"
    gitlab_minio_runner_cache_bucket_name: "runner-cache"
    gitlab_minio_runner_cache_keys_secret_name: "secret-gitlab-minio-runner-cache-keys"
    gitlab_minio_runner_cache_key_name: "gitlab-runner-cache-key"
    gitlab_minio_vpn_only_enabled: false
    gitlab_minio_service_url: "http://svc-gitlab-minio.ns-gitlab.svc.cluster.local:9000"

    # ------
    # GitLab
    # ------
    gitlab_namespace: "ns-gitlab"
    gitlab_image: "gitlab/gitlab-ce:16.11.10-ce.0"
    gitlab_domain: "gitlab-k8s.domain.com"
    gitlab_registry_domain: "gitlab-registry-k8s.domain.com"
    gitlab_pages_domain: "gitlab-pages-k8s.domain.com"
    gitlab_https_secret_name: "cert-gitlab-domain-com"
    gitlab_vpn_only_enabled: false
    gitlab_ssh_port_external: 3714
    gitlab_storage_class: "lh-major-multi-best-effort"
    gitlab_storage_class_logs: "lh-major-worker-multi-best-effort"
    gitlab_storage_config_size: "1Gi"
    gitlab_storage_data_size: "20Gi"
    gitlab_storage_logs_size: "5Gi"
    gitlab_resources_requests_memory: "2G"
    gitlab_resources_limits_memory: "3G"
    gitlab_root_password_secret_name: "secret-gitlab-root-password"

    # ------
    # ArgoCD
    # ------
    argocd_namespace: "argocd" # DO NOT CHANGE - hardcoded in ClusterRoleBindings
    argocd_ui_domain: "argocd-ui-k8s.domain.com"
    argocd_rpc_domain: "argocd-rpc-k8s.domain.com"
    argocd_https_secret_name: "cert-argocd-domain-com"
    argocd_vpn_only_enabled: false
    argocd_reconciliation_timeout: "30s"
    argocd_reconciliation_jitter: "10s"
    argocd_session_duration: "120h"
    argocd_gitlab_internal_url: "ssh://git@svc-gitlab.ns-gitlab.svc.cluster.local:22/server/git-ops.git"
    argocd_gitlab_ssh_key: |
      -----BEGIN OPENSSH PRIVATE KEY-----
      REPLACE_WITH_YOUR_KEY
      -----END OPENSSH PRIVATE KEY-----

    # ------
    # Helm timeouts
    # ------
    # Cilium
    cilium_helm_timeout: "5m"
    cilium_config_helm_timeout: "2m"
    cilium_post_config_helm_timeout: "2m"
    # Cert-manager
    cert_manager_helm_timeout: "5m"
    cert_manager_config_helm_timeout: "2m"
    # Traefik
    traefik_helm_timeout: "5m"
    # HAProxy
    haproxy_helm_timeout: "5m"
    haproxy_config_helm_timeout: "2m"
    # OLM
    olm_helm_timeout: "5m"
    # Medik8s
    medik8s_config_helm_timeout: "2m"
    # Longhorn
    longhorn_helm_timeout: "5m"
    longhorn_config_helm_timeout: "2m"
    # Infisical
    infisical_helm_timeout: "5m"
    infisical_config_helm_timeout: "2m"
    # External Secrets
    external_secrets_helm_timeout: "5m"
    # GitLab MinIO
    gitlab_minio_helm_timeout: "5m"
    # GitLab
    gitlab_helm_timeout: "10m"
    gitlab_config_helm_timeout: "2m"
    # ArgoCD
    argocd_helm_timeout: "5m"

    # ------
    # kubectl rollout/wait timeouts
    # ------
    # Cilium
    cilium_crds_timeout: "60s"
    cilium_daemonset_rollout_timeout: "300s"
    cilium_deployment_rollout_timeout: "120s"
    # Cert-manager
    cert_manager_crds_timeout: "60s"
    cert_manager_rollout_timeout: "180s"
    # Traefik
    traefik_crds_timeout: "60s"
    traefik_rollout_timeout: "300s"
    # HAProxy
    haproxy_crds_timeout: "60s"
    haproxy_rollout_timeout: "300s"
    # OLM
    olm_crds_timeout: "60s"
    olm_rollout_timeout: "300s"
    # Longhorn
    longhorn_crds_timeout: "60s"
    longhorn_manager_rollout_timeout: "300s"
    longhorn_ui_rollout_timeout: "120s"
    # Infisical
    infisical_postgres_rollout_timeout: "300s"
    infisical_redis_rollout_timeout: "300s"
    infisical_rollout_timeout: "300s"
    # External Secrets
    external_secrets_crds_timeout: "60s"
    external_secrets_rollout_timeout: "300s"
    # GitLab MinIO
    gitlab_minio_rollout_timeout: "300s"
    # GitLab
    gitlab_rollout_timeout: "600s"
    # ArgoCD
    argocd_crds_timeout: "60s"
    argocd_components_rollout_timeout: "120s"

  children:
    managers:
      hosts:
        k8s-manager-1:
          ansible_host: 158.160.1.1
          ansible_user: ubuntu
          ansible_password: "some-password-123"
          new_hostname: k8s-manager-1
          is_master: true
          # internal_ip - internal network interface IP (same as api_server_advertise_address for managers)
          internal_ip: "10.129.0.27"
          # api_server_advertise_address - IP for kube-apiserver to listen on and advertise to cluster
          # BareMetal: usually same as ansible_host (public IP)
          # Cloud (AWS/Yandex): usually private VPC IP (check with: ip route get 1)
          api_server_advertise_address: "158.160.1.1"
          api_server_bind_port: 6443
          longhorn_default_tags:
            - "lh-manager"
            - "lh-major-volume"

        # k8s-manager-2:
        #   ansible_host: 5.6.7.8
        #   ansible_user: root
        #   ansible_password: "some-password-456"
        #   new_hostname: k8s-manager-2
        #   api_server_advertise_address: "5.6.7.8"
        #   api_server_bind_port: 6443
        #   internal_ip: "5.6.7.8"
        #   longhorn_default_tags:
        #     - "lh-manager"
        #     - "lh-major-volume"

    workers:
      hosts:
        k8s-worker-1:
          ansible_host: 158.160.95.133
          ansible_user: ubuntu
          ansible_password: "some-password-789"
          new_hostname: k8s-worker-1
          internal_ip: "10.129.0.16"
          node_labels:
            - "node-role.kubernetes.io/worker="
          longhorn_default_tags:
            - "lh-worker"
            - "lh-major-volume"
            - "lh-minor-volume"

        # k8s-worker-2:
        #   ansible_host: 13.14.15.16
        #   ansible_user: root
        #   ansible_password: "some-password-101"
        #   new_hostname: k8s-worker-2
        #   node_labels:
        #     - "node-role.kubernetes.io/worker="
        #   longhorn_default_tags:
        #     - "lh-worker"
        #     - "lh-major-volume"
        #     - "lh-minor-volume"
